 

#### Gradient Descent

æ¢¯åº¦ä¸‹é™çš„ç›®æ ‡æ˜¯ä½¿æŸå¤±å‡½æ•°Læœ€å°åŒ–ï¼Œ$\theta^* = arg\ min\ L(\theta)$

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603152209444.png" alt="image-20200603152209444" style="zoom:50%; " />

#### Learning Rate

ä»$\theta_0$å¼€å§‹ï¼Œå…ˆè®¡ç®—å‡º$\theta_0$çš„æ¢¯åº¦ï¼Œå…¶ä¸­çº¢è‰²ç®­å¤´è¡¨ç¤ºæ¢¯åº¦çš„æ–¹å‘ï¼Œè“è‰²ç®­å¤´è¡¨ç¤ºç§»åŠ¨çš„æ–¹å‘ã€‚

<u>æ¢¯åº¦çš„æ–¹å‘æ˜¯å‡½æ•°å€¼åœ¨è¿™ä¸ªç‚¹å¢é•¿æœ€å¿«çš„æ–¹å‘ï¼Œæƒ³è¦ä½¿æŸå¤±å‡½æ•°Lçš„å€¼è¾¾åˆ°æœ€å°å€¼ï¼Œå°±å¿…é¡»è¦å¾€ç›¸åçš„æ–¹å‘è¿åŠ¨ã€‚</u>

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603152117920.png" alt="image-20200603152117920" style="zoom: 50%;" />

å­¦ä¹ ç‡ä¼šå‡ºç°ä»¥ä¸‹å››ç§ä¸åŒçš„æƒ…å†µï¼š

+ å­¦ä¹ ç‡å¤ªå°ï¼Œå³å›¾ä¸­è“è‰²çš„çº¿ï¼Œæ¯æ¬¡è·¨è¶Šçš„æ­¥é•¿å¾ˆå°å¾ˆå°ï¼Œæ¢¯åº¦æ¯æ¬¡å˜åŒ–çš„å€¼ä¹Ÿå°ï¼Œæ¨¡å‹è¦è¾¾åˆ°local minimaï¼Œå°±å¿…é¡»éœ€è¦æ›´å¤šçš„è®­ç»ƒæ—¶é—´ï¼›
+ å­¦ä¹ ç‡å¤ªå¤§ï¼Œå³å›¾ä¸­ç»¿è‰²çš„çº¿ï¼Œæ¯æ¬¡è·¨è¶Šçš„æ­¥é•¿ä¼šå¾ˆå¤§ï¼Œå¾ˆå¯èƒ½å½¢æˆåœ¨å±±è°·ä¹‹é—´éœ‡è¡çš„ç°è±¡ï¼›
+ å­¦ä¹ ç‡ç‰¹åˆ«å¤§ï¼Œå³å›¾ä¸­é»„è‰²çš„çº¿ï¼Œå°±å¾ˆå¯èƒ½ä¼šç›´æ¥è·³å‡ºlocal minimaï¼Œlossä¼šè¶Šæ¥è¶Šå¤§ï¼›
+ å­¦ä¹ ç‡åˆšå¥½åˆé€‚ï¼Œå³å›¾ä¸­çº¢è‰²çš„çº¿ï¼Œæ¯æ¬¡è·¨è¶Šçš„æ­¥é•¿éå¸¸åˆé€‚ï¼Œè¾¾åˆ°local minimaçš„æ—¶é—´ä¹Ÿä¸éœ€è¦ç‰¹åˆ«å¤šã€‚

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603161210664.png" alt="image-20200603161210664" style="zoom:50%;" />



ç”±äºæ‰‹åŠ¨è®¾ç½®learning rateä¼šå¯¼è‡´å¾ˆå¤šé—®é¢˜ï¼Œå°±å‡ºç°äº†ä¸€äº›è‡ªé€‚åº”çš„æ¢¯åº¦è°ƒæ•´æ–¹æ³•ã€‚

+ åˆšå¼€å§‹è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬ç¦»local minimumçš„è·ç¦»è¿˜å¾ˆè¿œï¼Œå› æ­¤å¯ä»¥ä½¿ç”¨ç¨å¤§çš„learning rateï¼›

+ åœ¨ç»è¿‡å¤šæ¬¡çš„è®­ç»ƒåï¼Œç¦»local minimumçš„è·ç¦»å·²ç»å¾ˆè¿‘äº†ï¼Œæ‰€ä»¥è¿™æ—¶å¯ä»¥ä½¿ç”¨å°çš„learning rateï¼›

+ åœ¨ç»è¿‡tæ¬¡çš„è®­ç»ƒåï¼Œlearning rateå¯ä»¥è¡°å‡ä¸º

  $\eta^t=\frac{\eta}{\sqrt{t+1}}$

#### Adagrad

> Divide the learning rate of each parameter by the **root mean square of its previous derivatives**

##### ç†è®ºæ¨å¯¼

ä½¿ç”¨è¿™ä¸ªå…¬å¼æ¥æ›´æ–°å‚æ•°wï¼Œ

$w^{t+1} \leftarrow  w^t - \frac{\eta^t}{\sigma^t}g^t$

å…¶ä¸­ï¼Œtè¡¨ç¤ºç¬¬tæ¬¡çš„updateï¼Œ$g^t = \frac{\partial L(\theta ^t)}{\partial w}$ï¼Œæ˜¯æŸå¤±å‡½æ•°Lå¯¹å‚æ•°wçš„å¯¼æ•°ï¼Œ$\sigma^t$è¡¨ç¤ºå…¶å…ˆå‰å¯¼æ•°çš„å‡æ–¹æ ¹ï¼ˆroot mean squareï¼‰

è®¡ç®—wçš„å…·ä½“ä¾‹å­å¦‚ä¸‹æ‰€ç¤ºï¼Œ

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603165047318.png" alt="image-20200603165047318" style="zoom: 50%; " />

å¾—å‡ºäº†$\sigma^t$å’Œ$\eta^t$çš„è¡¨è¾¾å¼åï¼Œå†å¸¦å…¥åŸå¼ï¼Œæ¶ˆé™¤åˆ†æ¯ä¸Šçš„$\sqrt{t+1}$å³å¯å¾—å‡ºä¸‹é¢çš„å…¬å¼ï¼Œ

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603165417281.png" alt="image-20200603165417281" style="zoom:50%;" />

##### Contradiction

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå¯¹äºä¸€èˆ¬çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆvanilla gradient descentï¼‰ï¼Œå½“æ¢¯åº¦gè¶Šå¤§æ—¶ï¼Œæ­¥é•¿å°±è¶Šå¤§ï¼›å¯¹äºAdagradï¼Œ   $g^t$åœ¨åˆ†å­ä¸Šï¼Œæ¢¯åº¦è¶Šå¤§æ­¥é•¿ä¹Ÿè¶Šå¤§ï¼Œ$\sum_{i=0}^t(g^i)^2$åœ¨åˆ†æ¯ä¸Šï¼Œæ•°å€¼è¶Šå¤§æ­¥é•¿ä¹Ÿå°±è¶Šå°ï¼Œçœ‹ä¼¼å‡ºç°äº†ä¸€ä¸ªçŸ›ç›¾ã€‚

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603172539601.png" alt="image-20200603172539601" style="zoom:50%;" />



æœ‰å­¦è€…å¯¹æ­¤ä¹Ÿåšå‡ºäº†è§£é‡Šï¼Œè®¤ä¸ºAdagradå¯ä»¥è§£é‡Š$g^t$å’Œ$\sum_{i=0}^t(g^i)^2$ä¹‹é—´çš„åå·®ï¼Œé€ æˆäº†åå·®çš„æ•ˆæœã€‚

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603173305074.png" alt="image-20200603173305074" style="zoom:50%;" />

**gradientè¶Šå¤§ï¼Œå‡½æ•°å€¼ç¦»minimaçš„è·ç¦»å°±è¶Šè¿œ**è¿™ä¸ªè¯´æ³•ä¸ä¸€å®šåœ¨æ‰€æœ‰æƒ…å†µä¸‹éƒ½æ˜¯æˆç«‹çš„ã€‚

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603174942400.png" alt="image-20200603174942400" style="zoom:50%;" />



å¯¹äºå·¦å›¾ä¸­çš„ä¸¤ä¸ªå‚æ•°w1å’Œw2ï¼Œç”»ä¸¤æ¡ç›´çº¿ï¼Œä¿æŒå…¶ä¸­ä¸€ä¸ªå˜é‡ä¸å˜ï¼Œå¾—å‡ºå¦ä¸€ä¸ªå˜é‡çš„å˜åŒ–æ›²çº¿ï¼Œåˆ†åˆ«å¯¹åº”å³å›¾ä¸­çš„æ›²çº¿ã€‚åœ¨å³å›¾ä¸­ï¼Œå¯¹äºw1ä¸­çš„aç‚¹å’Œw2ä¸­çš„cç‚¹ï¼Œcç‚¹è·ç¦»minimumçš„è·ç¦»æœ€è¿‘ï¼Œä½†æ¢¯åº¦å´æ›´å¤§ã€‚å› æ­¤åœ¨åˆ†ææ¢¯åº¦å’Œæ­¥é•¿æ—¶ï¼Œæˆ‘ä»¬ä¸èƒ½åªè€ƒè™‘ä¸€é˜¶å¯¼æ•°çš„å¤§å°ï¼Œè¿˜å¿…é¡»è¦è¦è€ƒè™‘äºŒé˜¶å¯¼æ•°çš„å¤§å°ï¼Œå³$y^{''}=2a$ã€‚

å³å›¾ä¸­çš„w1æ›²çº¿ï¼Œæ›²ç‡åŠå¾„æ¯”w2çš„æ›²çº¿æ›´å¤§ï¼Œä¸€é˜¶å¯¼æ•°å˜åŒ–å¾—æ›´å¹³ç¼“ï¼Œå› æ­¤äºŒé˜¶å¯¼æ•°çš„å˜åŒ–å°±æ¯”w2å¤§

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603175920345.png" alt="image-20200603175920345" style="zoom:50%;" />

å†æ¥å›é¡¾ä¸‹Adagradä¸­æ¯æ¬¡æ›´æ–°wçš„è¡¨è¾¾å¼ï¼Œ$w^{t+1}\leftarrow w^t - \frac{\eta}{\sqrt{\sum_{t=0}^t(g^i)^2}}g^t$

ä¸€é˜¶å¯¼æ•°ç”¨$g^t$è¡¨ç¤ºï¼ŒäºŒé˜¶å¯¼æ•°çš„å€¼åˆ™ç”¨åˆ†æ¯ä¸­çš„$\sum_{i=0}^t(g^i)^2$æ¥è¿›è¡Œè¯„ä¼°ï¼Œå³ä½¿ç”¨ä¸€é˜¶å¯¼æ•°çš„å€¼æ¥è¡¨ç¤ºäºŒé˜¶å¯¼æ•°çš„å€¼ã€‚

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603180425226.png" alt="image-20200603180425226" style="zoom:50%;" />

#### Stochastic Gradient Descent

å¯¹äºä¼ ç»Ÿçš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ŒæŸå¤±å‡½æ•°Lçš„è®¡ç®—åŒ…å«äº†æ‰€æœ‰çš„æ ·æœ¬;

éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ŒæŸå¤±å‡½æ•°$L^n$åˆ™åªä½¿ç”¨å…¶ä¸­ä¸€ä¸ªæ ·æœ¬ï¼Œè®¡ç®—æ•ˆç‡å¯ä»¥æé«˜å¾ˆå¤š

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603215238564.png" alt="image-20200603215238564" style="zoom:50%;" />

å¯¹æ¯”ç¤ºæ„å›¾å¦‚ä¸‹ï¼Œ

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603215557501.png" alt="image-20200603215557501" style="zoom:50%;" />

#### Feature scaling

##### åŸç†ä»‹ç»

> Make different features have the same scaling

ä½¿ä¸åŒé‡çº§çš„æ•°æ®é›†éƒ½å…·æœ‰ç›¸åŒçš„è§„æ¨¡ï¼Œæ¯”å¦‚x2çš„éƒ½æ˜¯å¤§äº100çš„å€¼ï¼Œç»è¿‡feature scalingï¼Œå°±å¯ä»¥ä½¿x2çš„æ•°å€¼èŒƒå›´å’Œx1ç›¸æ¥è¿‘ã€‚

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603220106332.png" alt="image-20200603220106332" style="zoom:50%;" />

x2å¯¹åº”w2ï¼Œx1å¯¹åº”w1ï¼Œå¯¹äºåŒä¸€ä¸ªw1ã€w2ï¼Œä½†x2çš„æ•°å€¼ä¸åŒ

åœ¨å·¦å›¾ä¸­ï¼Œç”±äºx1çš„æ•°å€¼ç›¸å¯¹äºx2æ¥è¯´éƒ½å¾ˆå°ï¼Œx1çš„å˜åŒ–å¯¹äºyæ¥è¯´å½±å“å¾ˆå°ï¼Œw1å¯¹yçš„å½±å“ä¹Ÿå¾ˆå°ï¼Œå¯¹lossçš„å½±å“ä¹Ÿå°ï¼Œå› æ­¤æ¢¯åº¦$\frac{\partial L}{\partial w_1}$åœ¨w1æ–¹å‘çš„å˜æ¢ä¹Ÿæ¯”è¾ƒå¹³ç¼“ï¼›x2çš„æ•°å€¼è¾ƒå¤§ï¼Œå¯¹lossçš„å½±å“ä¹Ÿå¤§ï¼Œå› æ­¤æ¢¯åº¦$\frac{\partial L}{\partial w_2}$åœ¨w2æ–¹å‘çš„å˜æ¢å°±æ¯”è¾ƒsharp

åœ¨å³å›¾ä¸­ï¼Œx1å’Œx2çš„è§„æ¨¡ï¼ˆscaleï¼‰æ˜¯æ¥è¿‘çš„ï¼Œå¯¹yçš„å½±å“ä¸ç›¸ä¸Šä¸‹ï¼Œå¯¹lossçš„å½±å“ä¹Ÿå·®ä¸å¤š

##### è®¡ç®—

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200603225755433.png" alt="image-20200603225755433" style="zoom:50%;" />

è®¡ç®—æ–¹å¼ï¼šç”¨$m_i$è¡¨ç¤ºå½“å‰æ ·æœ¬çš„å¹³å‡å€¼ï¼Œ$\sigma_i$ä¸ºå½“å‰æ ·æœ¬çš„æ ‡å‡†å·®ï¼Œiè¡¨ç¤ºç»´åº¦ï¼Œ$x_i^r$è¡¨ç¤ºç¬¬rä¸ªexampleï¼Œä½¿ç”¨å…¬å¼$x_i^r \leftarrow \frac{x_i^r-m_i}{\sigma_i}$æ¥è¿›è¡Œå½’ä¸€åŒ–è®¡ç®—

feature scalingå…¶å®å°±æ˜¯å°†æ¯ä¸€ä¸ªexampleéƒ½è¿›è¡Œå½’ä¸€åŒ–ï¼Œä½¿ä¹‹æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ$f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$
$$
\frac{X-\bar {X}}{\sqrt{D(x)}} \sim N(0,1)
$$

#### Gradient Descent Theory

##### Question

åœ¨æ±‚è§£æœ€å°åŒ–é—®é¢˜æ—¶ï¼Œ$\theta^* = arg\ min\ L(\theta)$ï¼Œæ¯æ¬¡æ›´æ–°$\theta$çš„å€¼ï¼Œå¹¶ä¸ä¸€å®šèƒ½ä½¿$ğ¿(\theta^0) >ğ¿(\theta^1)>ğ¿(\theta^2) >â‹¯$æˆç«‹

å¯¹äºç»™å‡ºçš„$\theta^1,\theta^2$ï¼Œæˆ‘ä»¬è¦å¦‚ä½•æ ¹æ®è¿™äº›å€¼æ¥æ‰¾å‡ºæœ€å°çš„lossï¼Ÿè¿™ä¹Ÿæ˜¯æˆ‘ä»¬æ¥ä¸‹æ¥ä¼šç ”ç©¶çš„é—®é¢˜

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604110917247.png" alt="image-20200604110917247" style="zoom:50%;" />

##### Talor Series

æ³°å‹’å…¬å¼å®šä¹‰å¦‚ä¸‹ï¼Œå°†å‡½æ•°h(x)åœ¨x=x0å¤„å±•å¼€
$$
h(x) = \sum_{k=0}^{\infty}\frac{h^{(k)}(x_0)}{k!}(x-x_0)^k
$$
å½“$x\rightarrow x_0$æ—¶ï¼Œè¡¨è¾¾å¼å¯å†™ä¸º$h(x)\approx h(x_0)+h'(x_0)(x-x_0)$

å¯¹äºäºŒå…ƒå‡½æ•°ï¼Œå½“$x\rightarrow x_0,y\rightarrow y_0$æ—¶ï¼Œç›¸åº”çš„è¡¨è¾¾å¼å¯ä»¥ç®€åŒ–ä¸º
$$
h(x,y)\approx h(x_0,y_0)+\frac{\partial h(x_0,y_0)}{\partial x}(x-x_0) + \frac{\partial h(x_0,y_0)}{\partial y}(y-y_0)
$$

##### Back to Formal Derivation

æŸå¤±å‡½æ•°losså¯ä»¥ç”¨ä»¥ä¸‹å…¬å¼è¡¨ç¤ºï¼Œ
$$
L(\theta)=L(a,b)+\frac{\partial L(a,b)}{\partial \theta_1}(\theta_1-a) +\frac{\partial L(a,b)}{\partial \theta_2}(\theta_2-b)
$$
ç®€åŒ–è¡¨è¾¾å½¢å¼ï¼Œä»¤$s=L(a,b),\ u=\frac{\partial L(a,b)}{\partial \theta_1},\ v=\frac{\partial L(a,b)}{\partial \theta_2}$

åˆ™$L(\theta)\approx s+u(\theta_1-a)+v(\theta_2-b)$

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨å›¾ä¸­red circleçš„èŒƒå›´å†…ï¼Œæ‰¾åˆ°$\theta_1,\theta_2$ï¼Œä½¿å¾—lossæœ€å°åŒ–ï¼Œè®¾red circleçš„åŠå¾„ä¸ºdï¼Œåœ†å¿ƒåæ ‡ä¸º(a,b)ï¼Œå°±æ–°å¢äº†ä¸€ä¸ªé™åˆ¶æ¡ä»¶$(\theta_1-a)^2+(\theta_2-b)^2\leq d^2$

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604114112061.png" alt="image-20200604114112061" style="zoom:50%;" />

ç”±äºsä¸$\theta_1,\theta_2$ä¸ç›¸å…³ï¼Œè¿™é‡ŒæŠŠsé¡¹å»æ‰ï¼Œ$L(\theta)$å¯ä»¥è½¬ä¸ºå†…ç§¯
$$
\begin{aligned}

L(\theta)_{min} & \approx   u(\theta_1-a)+v(\theta_2-b) \\
 & =  (u,v)\cdot(\theta_1-a,\theta_2-b) \\
 & =  (u,v)\cdot(\Delta \theta_1,\Delta\theta_2)
\end{aligned}
$$
å½“$(\Delta \theta_1,\Delta\theta_2)$ä¸$(u,v)$æ–¹å‘ç›¸åæ—¶ï¼Œä¸¤è€…çš„å†…ç§¯ä¸ºæœ€å°å€¼ï¼Œç”±äºä¸¤è€…çš„æ¨¡é•¿ä¸åŒï¼Œç”¨å‚æ•°$\eta$æ¥è¡¨ç¤ºä¸¤è€…ä¹‹é—´çš„å…³ç³»ã€‚

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604114656448.png" alt="image-20200604114656448" style="zoom:50%;" />

ç”±äº$\Delta\theta_1=\theta_1-a,\Delta\theta_2=\theta_2-b$ï¼Œåˆ™$\theta_1=a+\Delta\theta_1,\theta_2=b+\Delta\theta_2$ï¼Œå¯å¾—å‡º
$$
\begin{bmatrix} \theta_1 \\ \theta_2  \end{bmatrix}
=
\begin{bmatrix} a \\ b  \end{bmatrix}
- \eta \begin{bmatrix} u \\ v  \end{bmatrix}

=
\begin{bmatrix} a \\ b  \end{bmatrix}
- \eta \begin{bmatrix} \frac{\partial L(a,b)}{\partial \theta_1} \\ \frac{\partial L(a,b)}{\partial \theta_2}  \end{bmatrix}
$$

##### Limitation

åœ¨çœŸå®çš„å®éªŒç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬å¾€å¾€ä¼šè®¾ç½®ä¸€ä¸ªä¸´ç•Œå€¼ï¼ˆæ¯”å¦‚$10^{-4}$ï¼‰ï¼Œå½“è¯¥ç‚¹çš„æ¢¯åº¦å°äºè¯¥å€¼ï¼ˆå³$\approx 0$ï¼‰æ—¶ï¼Œå°±åœæ­¢è®­ç»ƒã€‚

å› æ­¤ï¼Œgradient descentçš„é™åˆ¶æ˜¯ï¼Œgradientä¸º0çš„ç‚¹å¹¶ä¸ä¸€å®šæ˜¯local minimumï¼Œè¿˜æœ‰å¯èƒ½æ˜¯saddle pointï¼Œä¹Ÿæœ‰å¯èƒ½æ˜¯æ¥è¿‘äº0çš„ç‚¹

<img src="https://gitee.com/scarleatt/image/raw/master/img/image-20200604123420086.png" alt="image-20200604123420086" style="zoom:50%;" />